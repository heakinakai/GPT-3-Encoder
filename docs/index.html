<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Home</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Home</h1>

    



    


    <h3> </h3>










    




    <section>
        <article><h4>This is a fork of https://github.com/latitudegames/GPT-3-Encoder. I made this fork so I could apply some PRs that had been sent to the upstream repo.</h4>
<pre><code>changelog: 
    add countTokens function
    add tokenStats function
    updated docs (npm run docs)
</code></pre>
<h1>GPT-3-Encoder</h1>
<p>Javascript BPE Encoder Decoder for GPT-2 / GPT-3</p>
<h2>About</h2>
<p>GPT-2 and GPT-3 use byte pair encoding to turn text into a series of integers to feed into the model. This is a javascript implementation of OpenAI's original python encoder/decoder which can be found <a href="https://github.com/openai/gpt-2">here</a></p>
<h2>Install with npm</h2>
<pre class="prettyprint source"><code>npm install @syonfox/gpt-3-encoder
</code></pre>
<h2>Usage</h2>
<p><a href="https://github.com/syonfox/GPT-3-Encoder#readme">View on GitHub</a></p>
<p><a href="https://syonfox.github.io/GPT-3-Encoder/">View Docs Pages</a></p>
<p>Compatible with Node &gt;= 12</p>
<pre class="prettyprint source lang-js"><code>
import {encode, decode, countTokens, tokenStats} from &quot;gpt-3-encoder&quot;
//or
const {encode, decode, countTokens, tokenStats} = require('gpt-3-encoder')

const str = 'This is an example sentence to try encoding out on!'
const encoded = encode(str)
console.log('Encoded this string looks like: ', encoded)

console.log('We can look at each token and what it represents')
for(let token of encoded){
  console.log({token, string: decode([token])})
}

//example count tokens usage
if(countTokens(str) > 5) {
    console.log(&quot;String is over five tokens, inconcevable&quot;);
}

const decoded = decode(encoded)
console.log('We can decode it back into:\n', decoded)

</code></pre>
<h2>Developers</h2>
<pre class="prettyprint source lang-sh"><code>git clone https://github.com/syonfox/GPT-3-Encoder.git

cd GPT-3-Encoder

npm install

npm run test
npm run docs

less Encoder.js

firefox ./docs/index.html

npm publish
</code></pre>
<h2>todo</h2>
<p>More stats that work well with this token representation.</p>
<p>Clean up and keep it simple.</p>
<p>more tests.</p>
<p>performance analysis</p>
<p>There are several performance improvements that could be made to the encode function:
(from gpt todo vet these recommendations)</p>
<pre><code>Cache the results of the encodeStr function to avoid unnecessary computation. You can do this by using a map or an object to store the results of encodeStr for each input string.
Use a regular expression to match the tokens in the input text instead of using the matchAll function. Regular expressions can be faster and more efficient than matchAll for certain types of patterns.
Use a different data structure to store the byte_encoder and encoder maps. Objects and maps can have different performance characteristics depending on the size and complexity of the data. You may want to experiment with different data structures to see which one works best for your use case.
Use a different data structure to store the bpe_tokens array. Arrays can be slower than other data structures for certain operations, such as appending new elements or concatenating multiple arrays. You may want to consider using a different data structure, such as a linked list or a queue, to store the bpe_tokens array.
Use a different algorithm to compute the BPE codes for the tokens. The current implementation of the bpe function may be inefficient for large datasets or for datasets with complex patterns. You may want to consider using a different algorithm, such as a divide-and-conquer or a hashing-based approach, to compute the BPE codes more efficiently.
</code></pre></article>
    </section>






</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Global</h3><ul><li><a href="global.html#bpe">bpe</a></li><li><a href="global.html#countTokens">countTokens</a></li><li><a href="global.html#decode">decode</a></li><li><a href="global.html#encode">encode</a></li><li><a href="global.html#tokenStats">tokenStats</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc/jsdoc">JSDoc 4.0.0</a> on Sun Dec 25 2022 12:15:33 GMT-0500 (Eastern Standard Time)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>